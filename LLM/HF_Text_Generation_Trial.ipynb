{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Basic pipeline to try causal inference from Huggingface pipeline directly"
      ],
      "metadata": {
        "id": "JdczqOMBp2VD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading"
      ],
      "metadata": {
        "id": "GRAy6KHQsoS8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOhvdLaHrmQe",
        "outputId": "b992a64f-2227-4ec5-dc55-c50bfa01825c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->bitsandbytes) (1.23.5)\n",
            "Installing collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.42.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers --upgrade\n",
        "!pip install accelerate\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "F-d1ganmrBe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jBtdKluxo7fb"
      },
      "outputs": [],
      "source": [
        "#Libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the open source available Text-Generation models in HF Model Cards\n",
        "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
      ],
      "metadata": {
        "id": "I4xqNPahqCsY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVUH5UDpqYhJ",
        "outputId": "b82e1954-8493-4c19-c2e7-7dac2a21b642"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
        "                                             cache_dir='./cache',\n",
        "                                             device_map='cuda:0',\n",
        "                                             load_in_4bit=True\n",
        "                                             #dtype = torch.float16\n",
        "                                             )"
      ],
      "metadata": {
        "id": "A2GJ0VjZqbXI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Causal Inference"
      ],
      "metadata": {
        "id": "FNIHIBwHvCX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "You are a Mathematician and for the given mathematical question return the answer as per the instructions given to you\n",
        "\n",
        "##\n",
        "Instructions :-\n",
        "1. Answer the given question only and do not make any assumptions on your own.\n",
        "2. Make the answer as short as possible\n",
        "3. Be as precise and concise as you can be.\n",
        "##\n",
        "\n",
        "Question :-\n",
        "\n",
        "1. Equation = {3x + 400 + 26y = 1000 + 40x}\n",
        "   For the above equation give me the value for x.\n",
        "\n",
        "2. A builder is constructing a roof.\n",
        "   The wood he is using for the sloped section of the roof is 4m long and the peak of the roof needs to be 2m high.\n",
        "   What angle should be the piece of wood make with the base of the roof ?\n",
        "\n",
        "Answer :-\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "iF_tU1S_qqcX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda:0\")"
      ],
      "metadata": {
        "id": "j5wXLr6Xubng"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "886WyL58whZ1",
        "outputId": "409ce58c-fe0d-4632-e51f-93cc89abe8fb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a Mathematician and for the given mathematical question return the answer as per the instructions given to you\n",
            "\n",
            "##\n",
            "Instructions :- \n",
            "1. Answer the given question only and do not make any assumptions on your own.\n",
            "2. Make the answer as short as possible\n",
            "3. Be as precise and concise as you can be.\n",
            "##\n",
            "\n",
            "Question :-\n",
            "\n",
            "1. Equation = {3x + 400 + 26y = 1000 + 40x}\n",
            "   For the above equation give me the value for x.\n",
            "\n",
            "2. A builder is constructing a roof.\n",
            "   The wood he is using for the sloped section of the roof is 4m long and the peak of the roof needs to be 2m high.\n",
            "   What angle should be the piece of wood make with the base of the roof ?\n",
            "\n",
            "Answer :-\n",
            "\n",
            "1. The value for x is 4.\n",
            "\n",
            "2. The angle is 45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comprehension"
      ],
      "metadata": {
        "id": "UTJEziXo3l9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "You are an expert question answering agent. Follow the instructions.\n",
        "\n",
        "##\n",
        "Instructions :-\n",
        "1. Read the given passage under \"Passage\" and answer the questions asked in \"Questions\".\n",
        "2. Be as precise and concise as you can.\n",
        "3. Answer to the question should be from the given passage only and nothing else.\n",
        "##\n",
        "\n",
        "##\n",
        "Passage\n",
        "\n",
        "Mike and Morris lived in the same village.\n",
        "While Morris owned the largest jewelry shop in the village, Mike was a poor farmer.\n",
        "Both had large families with many sons, daughters-in-law and grandchildren.\n",
        "One fine day, Mike, tired of not being able to feed his family, decided to leave the village and move to the city where he was certain to earn enough to feed everyone.\n",
        "Along with his family, he left the village for the city. At night, they stopped under a large tree.\n",
        "There was a stream running nearby where they could freshen up themselves.\n",
        "##\n",
        "\n",
        "\n",
        "Questions :-\n",
        "\n",
        "1. What was the job of Mike ?\n",
        "2. Describe family of Mike.\n",
        "Answer :-\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "VeRKJ3o3wkAf"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda:0\")"
      ],
      "metadata": {
        "id": "z7Omr-mq4YCE"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**inputs, max_new_tokens=20)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mPZ_P5h4Z3l",
        "outputId": "595ad962-5a62-4256-f586-c939c5e325a3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are an expert question answering agent. Follow the instructions.\n",
            "\n",
            "##\n",
            "Instructions :- \n",
            "1. Read the given passage under \"Passage\" and answer the questions asked in \"Questions\".\n",
            "2. Be as precise and concise as you can.\n",
            "3. Answer to the question should be from the given passage only and nothing else.\n",
            "##\n",
            "\n",
            "##\n",
            "Passage\n",
            "\n",
            "Mike and Morris lived in the same village. \n",
            "While Morris owned the largest jewelry shop in the village, Mike was a poor farmer. \n",
            "Both had large families with many sons, daughters-in-law and grandchildren. \n",
            "One fine day, Mike, tired of not being able to feed his family, decided to leave the village and move to the city where he was certain to earn enough to feed everyone. \n",
            "Along with his family, he left the village for the city. At night, they stopped under a large tree. \n",
            "There was a stream running nearby where they could freshen up themselves. \n",
            "##\n",
            "\n",
            "\n",
            "Questions :-\n",
            "\n",
            "1. What was the job of Mike ?\n",
            "2. Describe family of Mike.\n",
            "Answer :-\n",
            "\n",
            "1. Job of Mike - He was a poor farmer.\n",
            "2. Family of Mike -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What I can see from the question answering and causal answering part is that even for such basic Q/A a small model of 1B parameters is not that good unless the LLM formatted properly.\n",
        "\n",
        "Ultimately its a decoder only which can complete sentences, without prompting it to \"Answer\" it cannot give a complete answer.\n",
        "\n",
        "Generation can make sense but not for causal inference directly."
      ],
      "metadata": {
        "id": "c0fvVYqM645-"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f_rMfH1z8wnE"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}
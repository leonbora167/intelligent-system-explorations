{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a157363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c6ded9",
   "metadata": {},
   "source": [
    "# Stage 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96f6fd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages is :  228\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e308c46417564600966ff44485051380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a page text would be -> \n",
      " ‚Äî CHAPTER SEVENTEEN ‚Äî\n",
      "The Man with Two Faces\n",
      "It was Quirrell.\n",
      "‚ÄòYou!‚Äô gasped Harry.\n",
      "Quirrell smiled. His face wasn‚Äôt twitching at all.\n",
      "‚ÄòMe,‚Äô he said calmly. ‚ÄòI wondered whether I‚Äôd be meeting you\n",
      "here,\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "file_path = \"book1.pdf\"\n",
    "pages = []\n",
    "\n",
    "pdf = pdfplumber.open(file_path)\n",
    "length = len(pdf.pages)\n",
    "print(\"Number of pages is : \", length)\n",
    "\n",
    "with pdfplumber.open(file_path) as pdf:\n",
    "    for i, page in tqdm(enumerate(pdf.pages, start=1)):\n",
    "        text = page.extract_text()\n",
    "        pages.append({\"page_number\":i, \"text\":text})\n",
    "\n",
    "print(\"Example of a page text would be -> \\n\", pages[210][\"text\"][:200])\n",
    "\n",
    "text = pages[200][\"text\"][:350]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9beecda1",
   "metadata": {},
   "source": [
    "Need to detect Chapter Boundaries, Pages : Paragraphs separately ? Will see if frameworks internal tooling not strong enough\n",
    "\n",
    "* https://docs.langchain.com/oss/python/integrations/splitters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46724d12",
   "metadata": {},
   "source": [
    "## Programmatic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c63a46bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Through the Trapdoor 199\\ntogether so he couldn‚Äôt speak. Only his eyes were moving, looking\\nat them in horror.\\n‚ÄòWhat‚Äôve you done to him?‚Äô Harry whispered.\\n‚ÄòIt‚Äôs the full Body-Bind,‚Äô said Hermione miserably. ‚ÄòOh, Neville,\\nI‚Äôm so sorry.‚Äô\\n‚ÄòWe had to, Neville, no time to explain,‚Äô said Harry.\\n‚ÄòYou‚Äôll understand later, Neville,‚Äô said Ron, as they stepped']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import CharacterTextSplitter #Splitting text based on characters \n",
    "\n",
    "splitter = CharacterTextSplitter(chunk_size=50, chunk_overlap=5)\n",
    "chunks = splitter.split_text(text)\n",
    "print(chunks[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54711927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Through the Trapdoor 199\\ntogether so he couldn‚Äôt speak. Only his eyes were moving, looking\\nat them in horror.\\n‚ÄòWhat‚Äôve you done to him?‚Äô Harry whispered.\\n‚ÄòIt‚Äôs the full Body-Bind,‚Äô said Hermione miserably. ‚ÄòOh, Neville,\\nI‚Äôm so sorry.‚Äô\\n‚ÄòWe had to, Neville, no time to explain,‚Äô said Harry.\\n‚ÄòYou‚Äôll understand later, Neville,‚Äô said Ron, as they stepped']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "splitter = TokenTextSplitter(chunk_size=128, chunk_overlap=0)\n",
    "chunks = splitter.split_text(text)\n",
    "print(chunks)\n",
    "\n",
    "#Works on Tiktoken library, developed by openAI. So really helpful to decide the number of tokens for each chunk, could also specify the model name if required. Not sure how well it translates to other open source models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823e6296",
   "metadata": {},
   "source": [
    "* ‚ùî Could just do it hardcoded instead of library.\n",
    "* ‚ùî Consistency for all models or is it for only OpenAI models\n",
    "* ‚úîÔ∏è Easy to implement\n",
    "* ‚úîÔ∏è Can preserve partial context by overlapping chunks\n",
    "* ‚úîÔ∏è Good for explarotary analysis and quick preprocessing\n",
    "* ‚úîÔ∏è Simple keyword matching\n",
    "* ‚ùå Ignores meaning - may cut sentences completely\n",
    "\n",
    "* üóíÔ∏è These are Length Based strategies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "321367c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Through the Trapdoor 199\\ntogether so he couldn‚Äôt speak. Only his eyes were moving, looking\\nat them in horror.', '‚ÄòWhat‚Äôve you done to him?‚Äô Harry whispered.\\n‚ÄòIt‚Äôs the full Body-Bind,‚Äô said Hermione miserably. ‚ÄòOh, Neville,', 'I‚Äôm so sorry.‚Äô\\n‚ÄòWe had to, Neville, no time to explain,‚Äô said Harry.', '‚ÄòYou‚Äôll understand later, Neville,‚Äô said Ron, as they stepped']\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=120, chunk_overlap=10)\n",
    "chunks = splitter.split_text(text)\n",
    "print(chunks)\n",
    "\n",
    "#For languages like Japanese which do not have defined word boundaries would need to change the seperators to custom ones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf8eb84",
   "metadata": {},
   "source": [
    "* üóíÔ∏è Text Structure Based Strategy\n",
    "* üóíÔ∏è Attempst to keep largert units like paragraphs as intact as possible. If unit exceeds chunk size it moves to next level (sentences), process continues to word level if necessary\n",
    "* ‚ùå Will most probably produce variable sized chunks that are harder to manage/index.\n",
    "* ‚ùå Slightly more complex \n",
    "* ‚úîÔ∏è Handles nested structures like paragraphs and/or sections \n",
    "* ‚úîÔ∏è Better context handling than fixed size splitting\n",
    "* ‚úîÔ∏è Tries to adpat to different levels of text granularity and create splits that maintain natural language flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "789f0cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header1\"),\n",
    "    ('##', 'Header2')\n",
    "]\n",
    "\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "chunks = splitter.split_text(text)\n",
    "\n",
    "import json \n",
    "from langchain_text_splitters import RecursiveJsonSplitter \n",
    "\n",
    "from langchain_text_splitters import Language\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "python_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON, chunk_size=50, chunk_overlap=0)\n",
    "\n",
    "from langchain_text_splitters import HTMLHeaderTextSplitter\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\")\n",
    "]\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "chunks = html_splitter.split_text(\"Text ll be here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9bd78",
   "metadata": {},
   "source": [
    "* ‚úèÔ∏è Have not done anything on the previous examples as its related to Markdown, HTML Tags or creating splits based on code functions which are not in the scope of the novel pdf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79e10fd",
   "metadata": {},
   "source": [
    "## Embedding based chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8280811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\prish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\prish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import nltk \n",
    "import numpy as np \n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5d7691",
   "metadata": {},
   "source": [
    "### Semantic Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66af8495",
   "metadata": {},
   "source": [
    "* Semantic chunking is about splitting text based on meaning and not fixed sizes\n",
    "* So the split should happen when the topic \"changes\" or when the max length for each chunk is reached\n",
    "* So fixed chunks with multiple meanings should be solved \n",
    "* ‚ùå getting long paragraphs with multiple meanings and contexts\n",
    "* poor chunks ‚ùå with irrelevant boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59385da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "model_id = \"KaLM-Embedding/KaLM-embedding-multilingual-mini-instruct-v2.5\"\n",
    "model_arguments = {\"torch_dtype\":torch.bfloat16,\n",
    "                   \"device_map\":\"cuda\"\n",
    "                   #\"attn_implementation\":\"flash_attention_2\"\n",
    "                   }\n",
    "\n",
    "model = SentenceTransformer(model_id,\n",
    "                            cache_folder = \"models\",\n",
    "                            model_kwargs = model_arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a8c5d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e747e846c5c4d48ba9e3ebaf17ec6c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences are  4863\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff4f4166ccf4106a3fea7fd347f8b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every embedding is of shape (4863, 896)\n"
     ]
    }
   ],
   "source": [
    "with open(\"book.txt\", \"w\") as w:\n",
    "    for i in tqdm(pages):\n",
    "        w.write(i[\"text\"])\n",
    "\n",
    "text = open(\"book.txt\").read()\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Number of sentences are \", len(sentences))\n",
    "\n",
    "embs = model.encode(sentences, \n",
    "                    convert_to_numpy = True, show_progress_bar=True)\n",
    "\n",
    "print(f\"Every embedding is of shape {embs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6822e7ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b9c7cf4e8f4998ad6c14b2002a4eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4862 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def cosine(a, b):\n",
    "    cosine_similarity = np.dot(a,b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    return cosine_similarity\n",
    "\n",
    "similarities = []\n",
    "for i in tqdm(range(len(embs)-1)):\n",
    "    cosine_similarity = cosine(embs[i], embs[i+1])\n",
    "    similarities.append(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25808816",
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = [0] #we ll start from 0 as the starting boundary\n",
    "threshold = 0.70\n",
    "for index, similarity in enumerate(similarities):\n",
    "    if similarity < threshold:\n",
    "        boundaries.append(index+1)\n",
    "\n",
    "boundaries.append(len(sentences)) #Adding the last boundary limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0de5e028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created are ->  3198\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for i in range(len(boundaries)-1):\n",
    "    start = boundaries[i]\n",
    "    end = boundaries[i+1]\n",
    "    chunk = \" \".join(sentences[start:end])\n",
    "    chunks.append(chunk)\n",
    "print(\"Number of chunks created are -> \", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdc72a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When a letter arrives for unhappy but\\nordinary Harry Potter, a decade-old secret\\nis revealed to him.',\n",
       " 'His parents were\\nwizards, killed by a Dark Lord‚Äôs curse\\nwhen Harry was just a baby, and which he\\nsomehow survived.',\n",
       " 'Escaping from his\\nunbearable Muggle guardians to Hogwarts,\\na wizarding school brimming with ghosts\\nand enchantments, Harry stumbles into a\\nsinister adventure when he finds a three-\\nheaded dog guarding a room on the third\\nfloor.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409a03bc",
   "metadata": {},
   "source": [
    "### Agentic Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ec08d4",
   "metadata": {},
   "source": [
    "* Use an LLM to :-\n",
    "    * Read the document\n",
    "    * Understand it\n",
    "    * Understand the structure, topics, boundaries etc. \n",
    "    * Determine how to chunk adaptively \n",
    "    * Sometime generate hierarchial or multilayer chunks \n",
    "    * Chunk differently depending on the end use case\n",
    "\n",
    "* Call it kind of a __task aware__ chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2923ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "torch.random.manual_seed(0)\n",
    "\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir = \"models\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    cache_dir = \"models\",\n",
    "    dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ") \n",
    "#Context length for this odel is 32,768\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c1c325f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an expert text segmentation agent. \n",
    "Your task is to read the given text and split it into meaningful, semantically coherent chunks.\n",
    "\n",
    "Rules :- \n",
    "1. Use natural boundaries \n",
    "2. No fixed size \n",
    "3. Consider topic changes \n",
    "4. Keep chunks at max size of 1000 tokens\n",
    "5. Output format should be in a Json format \n",
    "\n",
    "[{\"chunk\" : \"1\", \n",
    "\"text\" : \"...\"} ]\n",
    "\n",
    "The text to be read is given below \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "784be1c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks are  62\n"
     ]
    }
   ],
   "source": [
    "#Taking an assumption that tokens ~ number of chars/4\n",
    "\n",
    "chars_per_chunk = 20000 # 20000/4 = 5000 Tokens approx\n",
    "current = []\n",
    "current_len = 0\n",
    "chunks = []\n",
    "\n",
    "for index, sent in enumerate(sentences):\n",
    "    current.append(sent)\n",
    "    current_len += len(sent)\n",
    "    if(current_len >= 7000):\n",
    "        chunks.append(\" \".join(current))\n",
    "        current_len = 0\n",
    "        current = []\n",
    "\n",
    "print(\"Number of chunks are \", len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39a5c758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac2efadd69d74b6bb431dd8f1e8d1b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = []\n",
    "for i in tqdm(range(len(chunks))):\n",
    "    text = chunks[i]\n",
    "    final_prompt = prompt + \"\\n\" + text\n",
    "    messages = [{\"role\":\"user\", \"content\":final_prompt}]\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "                                        messages,\n",
    "                                        tokenize=False,\n",
    "                                        add_generation_prompt=True,\n",
    "                                        enable_thinking=True # Switches between thinking and non-thinking modes. Default is True\n",
    "                                        )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    generated_ids = model.generate(**model_inputs, \n",
    "                                   max_new_tokens=32768)\n",
    "    output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    try:\n",
    "        # rindex finding 151668 (</think>)\n",
    "        index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "    except ValueError:\n",
    "        index = 0\n",
    "\n",
    "    thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "    content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "    results.append(content)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "213cfe16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'chunk': '1',\n",
       "  'text': \"When a letter arrives for unhappy but ordinary Harry Potter, a decade-old secret is revealed to him. His parents were wizards, killed by a Dark Lord‚Äôs curse when Harry was just a baby, and which he somehow survived. Escaping from his unbearable Muggle guardians to Hogwarts, a wizarding school brimming with ghosts and enchantments, Harry stumbles into a sinister adventure when he finds a three-headed dog guarding a room on the third floor. Then he hears of a missing stone with astonishing powers which could be valuable, dangerous, or both. 'Funny, imaginative, magical ... Rowling has woken up a whole generation to reading. In the 2020s, thirty-something book-lovers will know each other by smug references to Diagon Alley and Quidditch' The Times 'This is a terrific book' Sunday Telegraph 'Has all the makings of a classic ... Rowling uses classic narrative devices with flair and originality and delivers a complex and demanding plot in the form of a hugely entertaining thriller' Scotsman 'And you thought wizardry was for children. Harry Potter will make you think again. He casts his spells on grown-up too' James Naughtie 'Full of surprises and jokes; comparisons with Dahl are, this time, justified' Sunday Times ¬£11.99 Harry Potter and the Philosopher‚Äôs Stone Titles available in the Harry Potter series (in reading order): Harry Potter and the Philosopher‚Äôs Stone Harry Potter and the Chamber of Secrets Harry Potter and the Prisoner of Azkaban Harry Potter and the Goblet of Fire Harry Potter and the Order of the Phoenix Harry Potter and the Half-Blood Prince Harry Potter and the Deathly Hallows Titles available in the Harry Potter series (in Latin): Harry Potter and the Philosopher‚Äôs Stone Harry Potter and the Chamber of Secrets (in Welsh, Ancient Greek and Irish): Harry Potter and the Philosopher‚Äôs Stone J. K. Rowling All rights reserved; no part of this publication may be reproduced or transmitted by any means, electronic, mechanical, photocopying or otherwise, without the prior permission of the publisher First published in Great Britain in 1997 Bloomsbury Publishing Plc, 36 Soho Square, London, W1D 3QY This digital edition should have been published by Pottermore Limited in 2012 Copyright ¬© 1997 J. K. Rowling Harry Potter, names, characters and related indicia are copyright and trademark Warner Bros., 2000‚Ñ¢ The moral right of the author has been asserted A CIP catalogue record of this book is available from the British Library ISBN 978 0 7475 7360 9 The paper this book is printed on is certified by the ¬© 1996 Forest Stewardship Council A.C. (FSC). It is ancient-forest friendly. The printer holds FSC chain of custody SGS-COC-2061. ¬© FSC Mixed Sources Product group from well-managed forests and other controlled sources Cert no. SGS-COC-2061 www.fsc.org ¬©1996 Forest Stewardship Council Printed in Great Britain by Clays Ltd, St Ives plc Typeset by Dorchester Typesetting 5 7 9 10 8 6 4 www.bloomsbury.com/harrypotterfor Jessica, who loves stories, for Anne, who loved them too, and for Di, who heard this one first.‚Äî CHAPTER ONE ‚Äî The Boy Who Lived Mr and Mrs Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you‚Äôd expect to be involved in anything strange or mysterious, because they just didn‚Äôt hold with such nonsense. Mr Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large moustache. Mrs Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbours. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere. The Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn‚Äôt think they could bear it if anyone found out about the Potters. Mrs Potter was Mrs Dursley‚Äôs sister, but they hadn‚Äôt met for several years; in fact, Mrs Dursley pretended she didn‚Äôt have a sister, because her sister and her good-for-nothing husband were as unDursleyish as it was possible to be. The Dursleys shuddered to think what the neighbours would say if the Potters arrived in the street. The Dursleys knew that the Potters had a small son, too, but they had never even seen him. This boy was another good reason for keeping the Potters away; they didn‚Äôt want Dudley mixing with a child like that. When Mr and Mrs Dursley woke up on the dull, grey Tuesday our story starts, there was nothing about the cloudy sky outside to suggest that strange and mysterious things would soon be happening all over the country. Mr Dursley hummed as he picked out his most boring tie for work and Mrs Dursley gossiped away Harry Potter happily as she wrestled a screaming Dudley into his high chair. None of them noticed a large tawny owl flutter past the window. At half past eight, Mr Dursley picked up his briefcase, pecked Mrs Dursley on the cheek and tried to kiss Dudley goodbye but missed, because Dudley was now having a tantrum and throwing his cereal at the walls. 'Little tyke,' chortled Mr Dursley as he left the house. He got into his car and backed out of number four‚Äôs drive. It was on the corner of the street that he noticed the first sign of something peculiar ‚Äì a cat reading a map. For a second, Mr Dursley didn‚Äôt realize what he had seen ‚Äì then he jerked his head around to look again. There was a tabby cat standing on the corner of Privet Drive, but there wasn‚Äôt a map in sight. What could he have been thinking of? It must have been a trick of the light. Mr Dursley blinked and stared at the cat. It stared back. As Mr Dursley drove around the corner and up the road, he watched the cat in his mirror. It was now reading the sign that said Privet Drive ‚Äì no, looking at the sign; cats couldn‚Äôt read maps or signs. Mr Dursley gave himself a little shake and put the cat out of his mind. As he drove towards town he thought of nothing except a large order of drills he was hoping to get that day. But on the edge of town, drills were driven out of his mind by something else. As he sat in the usual morning traffic jam, he couldn‚Äôt help noticing that there seemed to be a lot of strangely dressed people about. People in cloaks. Mr Dursley couldn‚Äôt bear people who dressed in funny clothes ‚Äì the get-ups you saw on young people! He supposed this was some stupid new fashion. He drummed his fingers on the steering wheel and his eyes fell on a huddle of these weirdos standing quite close by. They were whispering excitedly together. Mr Dursley was enraged to see that a couple of them weren‚Äôt young at all; why, that man had to be older than he was, and wearing an emerald-green cloak! The nerve of him! But then it struck Mr Dursley that this was probably some silly stunt ‚Äì these people were obviously collecting for something ... yes, that would be it.\"}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(results[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b8ab9f",
   "metadata": {},
   "source": [
    "* Need to evaluate more on the context length and type of llm model\n",
    "* Basic gist is that it should be able to receive the large text that is snippets or parts from the book and create chunks out of it\n",
    "* The json format would make it easier further downstream to handle the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6857d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

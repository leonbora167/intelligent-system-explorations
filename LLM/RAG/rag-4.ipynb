{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b58c69",
   "metadata": {},
   "source": [
    "# Attribution + Hallucination Control + Long Context Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bfcef9",
   "metadata": {},
   "source": [
    "1. Attribution -> Where did this answer come from \n",
    "2. Hallucination Control -> Is the answer supported by the retrieved text \n",
    "3. Long Context handling -> Reasoning over more text without breaking | Losing context windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf53c01e",
   "metadata": {},
   "source": [
    "* Return exact span and source of the retrieved texts, evidence based QA \n",
    "* Change the prompting to ensure whatever it answers its only from the retrieved texts\n",
    "* Structured output for better response quality \n",
    "\n",
    "| Extractive QA vs Generative QA\n",
    "| Evidence grounded generation \n",
    "| Chain of verification prompts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef3dcc5",
   "metadata": {},
   "source": [
    "* Prompt level verification\n",
    "* Self verification pass (pass through llm again)\n",
    "* Claim decomposition - break answer into atomic claims \n",
    "* Retrieval based validation \n",
    "\n",
    "| Faithfulness vs Relevance | Self-consistency | Verifier LLM's | RAG Hallucination Taxonomy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f03ce4",
   "metadata": {},
   "source": [
    "* Map Reduce -> Split context - Anwer per chunk - Aggregate \n",
    "* Hierarchial Rag -> Chunk - Summarize - retrieve summaries - expand\n",
    "* Sliding window/Streaming RAG -> LLM reasons incrementally instead of all at once \n",
    "* Check long context models and their accuracy vs context length for particular tasks\n",
    "\n",
    "| Local Inference Runtimes | Context Window vs VRAM Tradeoffs | Chunk Routing instead of brute force context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f1b97c",
   "metadata": {},
   "source": [
    "**Metrics to track**\n",
    "\n",
    "* Hallucination Rate \n",
    "* Citation Accuracy \n",
    "* Coverage \n",
    "* Human Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a47ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter, CharacterTextSplitter, SentenceTransformersTokenTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_classic.schema import Document\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_classic.retrievers import EnsembleRetriever\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever \n",
    "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker \n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from sentence_transformers import CrossEncoder\n",
    "import json, os\n",
    "from langchain_huggingface import HuggingFacePipeline \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_classic.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0924b124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded documents are  228\n",
      "Example of one document \n",
      "  34 Harry Potter  \n",
      " \n",
      "been trying to do. He shouted at Harry for about half an hour and \n",
      "then told him to go and make a cup of tea. Harry shuffled miser-\n",
      "ably off into the kitchen, and by the time he got back, the post  \n",
      "had arrived, right into Uncle V\n",
      "Chunks created  1057\n"
     ]
    }
   ],
   "source": [
    "#Load pdf\n",
    "pdf_path = \"book1.pdf\"\n",
    "pdf = PyPDFLoader(pdf_path)\n",
    "docs = pdf.load() \n",
    "print(\"Number of loaded documents are \", len(docs))\n",
    "print(\"Example of one document \\n \", docs[35].page_content[:250])\n",
    "\n",
    "#Create chunks \n",
    "chunk_size = 600 \n",
    "chunk_overlap = 150 \n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,\n",
    "                                          chunk_overlap=chunk_overlap,\n",
    "                                          separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "                                          )\n",
    "chunked_docs = splitter.split_documents(docs)\n",
    "print(\"Chunks created \", len(chunked_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bbf0816",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, doc in enumerate(chunked_docs):\n",
    "    doc.metadata[\"chunk_id\"] = f\"p{doc.metadata['page']}_c{index}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d29214a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'Acrobat Distiller 7.0.5 (Windows)',\n",
       " 'creator': 'PScript5.dll Version 5.2',\n",
       " 'creationdate': '2012-05-01T23:57:27+10:00',\n",
       " 'subject': \"When a letter arrives for unhappy but ordinary Harry Potter, a decade-old secret is revealed to him. His parents were wizards, killed by a Dark Lord's curse when Harry was just a baby, and which he somehow survived. Escaping from his unbearable Muggle guardians to Hogwarts, a wizarding school brimming with ghosts and enchantments, Harry stumbles into a sinister adventure when he finds a threeheaded dog guarding a room on the third floor. Then he hears of a missing stone with astonishing powers which could be valuable, dangerous, or both.\",\n",
       " 'author': 'J.K. Rowling',\n",
       " 'keywords': '',\n",
       " 'moddate': '2012-07-01T14:20:39+10:00',\n",
       " 'title': 'Harry Potter and the Philosopher’s Stone',\n",
       " 'source': 'book1.pdf',\n",
       " 'total_pages': 228,\n",
       " 'page': 8,\n",
       " 'page_label': '9',\n",
       " 'chunk_id': 'p8_c10'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_docs[10].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9be6027",
   "metadata": {},
   "outputs": [],
   "source": [
    "persona = '''You are a literary analyst assistant. Your work is to analyze context of books given to you and answer questions accurately'''\n",
    "\n",
    "instructions = '''\n",
    "Rules:\n",
    "1. Use only the context given below\n",
    "2. Every factual claim by you must be supported by a direct quote\n",
    "3. Attach the chunk_id for each quote\n",
    "4. If the answer to the question is not fully supported, reply back by saying \"Answer not given in the current context\"\n",
    "\n",
    "'''\n",
    "\n",
    "answer_format = '''\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Return your answer in this JSON format:\n",
    "\n",
    "{{\n",
    "  \"answer\": \"...\",\n",
    "  \"claims\": [\n",
    "    {{\n",
    "      \"claim\": \"...\",\n",
    "      \"evidence\": {{\n",
    "        \"quote\": \"...\",\n",
    "        \"chunk_id\": \"...\"\n",
    "      }}\n",
    "    }}\n",
    "  ],\n",
    "  \"confidence\": <return confidence of your answer here upto the first decimal point>\n",
    "}}\n",
    "'''\n",
    "\n",
    "prompt_template = f'''\n",
    "{persona}\n",
    "\n",
    "{instructions}\n",
    "\n",
    "{answer_format}\n",
    "''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bc10eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_prompt = PromptTemplate(\n",
    "    input_variables = [\"context\", \"question\"],\n",
    "    template=prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d25004a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load embeddings model\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name = \"KaLM-Embedding/KaLM-embedding-multilingual-mini-instruct-v2.5\",\n",
    "                                         model_kwargs = {\"device\" : \"cuda\"},\n",
    "                                         encode_kwargs = {\"normalize_embeddings\" : True})\n",
    "\n",
    "#Create vector store from embeddings model \n",
    "vectorstore = FAISS.from_documents(chunked_docs, embedding=embeddings_model)\n",
    "vectorstore.save_local(\"faiss_book1_index\")\n",
    "\n",
    "# Creating retriever for chain operations \n",
    "faiss_retriever = vectorstore.as_retriever(search_type=\"similarity\",\n",
    "                                           search_kwargs = {\"k\":3})\n",
    "\n",
    "#Load BM25 Retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(chunked_docs)\n",
    "bm25_retriever.k =5 \n",
    "\n",
    "hybrid_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever],\n",
    "                                     weights = [0.4, 0.6] #FAISS Gets more weights -> check why\n",
    ")\n",
    "\n",
    "#Create the reranker\n",
    "reranker_model = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-v2-m3\",\n",
    "                                         model_kwargs = {\"device\" : \"cuda\"})\n",
    "reranker =  CrossEncoderReranker(model = reranker_model,\n",
    "                                top_n=3)\n",
    "\n",
    "#Wrap hybrid retriever \n",
    "compression_retriever = ContextualCompressionRetriever(base_retriever=hybrid_retriever,\n",
    "                                                       base_compressor=reranker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eacf9090",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda\n"
     ]
    }
   ],
   "source": [
    "# Loading the llm model to implement on top of this RAG\n",
    "model_name = \"Qwen/Qwen3-0.6B\" \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=\"models\")\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "                                             device_map=\"cuda\",\n",
    "                                             cache_dir=\"models\",\n",
    "                                             dtype=torch.bfloat16)\n",
    "\n",
    "pipe = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer = tokenizer,\n",
    "                max_new_tokens = 256,\n",
    "                temperature = 0.6)\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e523622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question, retriever, llm):\n",
    "    docs = compression_retriever.invoke(question) #Return the final context docs which should be mix of BM25 + FAISS Embeddings + Reranking\n",
    "    context = \"\\n\\n\".join(f\"Chunk ID - [{d.metadata['chunk_id']}] \\n{d.page_content}\" for d in docs)\n",
    "    prompt = answer_format.format(context=context, question=question)\n",
    "    response = llm.invoke(prompt)\n",
    "    return response, docs, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bbf4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "verifier_prompt_template = '''\n",
    "You are a strict fact checker.\n",
    "\n",
    "Check whether ALL claims in the answer are fully supported by the context.\n",
    "\n",
    "Rules:\n",
    "- A claim is supported ONLY if the quote logically entails it.\n",
    "- If any claim is unsupported, mark the answer as unsupported.\n",
    "\n",
    "Return JSON:\n",
    "\n",
    "{{\n",
    "  \"supported\": true/false,\n",
    "  \"unsupported_claims\": [],\n",
    "  \"confidence\": <return confidence of your answer here upto the first decimal point>\n",
    "}}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "{answer}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "'''\n",
    "\n",
    "verifier_prompt = PromptTemplate(input_variables=[\"question\", \"answer\", \"context\"],\n",
    "                                 template=verifier_prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84eb3886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_answer(question, answer_json, docs, llm):\n",
    "    context = \"\\n\\n\".join(f\"{d.metadata['chunk_id']} \\n{d.page_content}\" for d in docs) \n",
    "    prompt = verifier_prompt.format(question=question, answer=answer_json, context=context) \n",
    "    verification = llm.invoke(prompt)\n",
    "    return verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3093a2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Answer is: \n",
      "------------ \n",
      " \n",
      "Context:\n",
      "Chunk ID - [p222_c1041] \n",
      "McGonagall’s giant chess set!’ \n",
      "At last there was silence again. \n",
      "‘Second – to Miss Hermione Granger … for the use of cool logic \n",
      "in the face of fire, I award Gryffindor house fifty points.’ \n",
      "Hermione buried her face in her arms; Harry strongly suspected \n",
      "she had burst into tears. Gryffindors up and down the table were \n",
      "beside themselves – they were a hundred points up. \n",
      "‘Third – to Mr Harry Potter …’ said Dumbledore. The room \n",
      "went deadly quiet. ‘… for pure nerve and outstanding courage, I \n",
      "award Gryffindor house sixty points.’ \n",
      "The din was deafening. Those who could add up while yelling\n",
      "\n",
      "Chunk ID - [p80_c367] \n",
      "‘Am I?’ said Harry, feeling dazed. \n",
      "‘Goodness, didn’t you know, I’d have found out everything I \n",
      "could if it was me,’ said Hermio ne. ‘Do either of you know what \n",
      "house you’ll be in? I’ve been asking around and I hope I’m in \n",
      "Gryffindor, it sounds by far the best, I hear Dumbledore himself\n",
      "\n",
      "Chunk ID - [p92_c417] \n",
      "The Sorting Hat  91 \n",
      " \n",
      "Slytherin, not Slytherin.’ \n",
      "‘Not Slytherin, eh?’ said the small voice. ‘Are you sure? You \n",
      "could be great, you know, it’s all here in your head, and Slytherin \n",
      "will help you on the way to greatn ess, no doubt about that – no? \n",
      "Well, if you’re sure – better be GRYFFINDOR!’ \n",
      "Harry heard the hat shout the last  word to the whole Hall. He \n",
      "took off the hat and walked shakily  towards the Gryffindor table. \n",
      "He was so relieved to have been chosen and not put in Slytherin, \n",
      "he hardly noticed that he was ge tting the loudest cheer yet. Percy\n",
      "\n",
      "Question:\n",
      "Which house was Harry sorted into ?\n",
      "\n",
      "Return your answer in this JSON format:\n",
      "\n",
      "{\n",
      "  \"answer\": \"...\",\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"claim\": \"...\",\n",
      "      \"evidence\": {\n",
      "        \"quote\": \"...\",\n",
      "        \"chunk_id\": \"...\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"confidence\": <return confidence of your answer here upto the first decimal point>\n",
      "}\n",
      "Answer:\n",
      "{\n",
      "  \"answer\": \"Gryffindor\",\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"claim\": \"Harry was sorted into Gryffindor\",\n",
      "      \"evidence\": {\n",
      "        \"quote\": \"The Sorting Hat  91 - Slytherin, not Slytherin.\",\n",
      "        \"chunk_id\": \"p92_c417\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"confidence\": 0.9\n",
      "}\n",
      "Answer:\n",
      "{\n",
      "  \"answer\": \"Gryffindor\",\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"claim\": \"Harry was sorted into Gryffindor\",\n",
      "      \"evidence\": {\n",
      "        \"quote\": \"The Sorting Hat  91 - Slytherin, not Slytherin.\",\n",
      "        \"chunk_id\": \"p92_c417\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"confidence\": 0.9\n",
      "}\n",
      "Answer:\n",
      "{\n",
      "  \"answer\": \"Gryffindor\",\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"claim\": \"Harry was sorted into Gryffindor\",\n",
      "      \"evidence\": {\n",
      "        \"quote\": \"The Sorting Hat  91 - Slytherin, not Slytherin.\",\n",
      "        \"chunk_id\": \"p\n",
      "\n",
      "--------------\n",
      "Retrieved docs are : \n",
      "\n",
      "McGonagall’s giant chess set!’ \n",
      "At last there was silence again. \n",
      "‘Second – to Miss Hermione Granger\n",
      "‘Am I?’ said Harry, feeling dazed. \n",
      "‘Goodness, didn’t you know, I’d have found out everything I \n",
      "cou\n",
      "The Sorting Hat  91 \n",
      " \n",
      "Slytherin, not Slytherin.’ \n",
      "‘Not Slytherin, eh?’ said the small voice. ‘Are y\n"
     ]
    }
   ],
   "source": [
    "question = \"Which house was Harry sorted into ?\"\n",
    "\n",
    "answer, docs, context = generate_answer(question, compression_retriever, llm)\n",
    "print(\"LLM Answer is: \\n------------ \\n\", answer)\n",
    "print(\"\\n--------------\\nRetrieved docs are : \\n\")\n",
    "for i in docs:\n",
    "    print(i.page_content[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2fc0368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are a strict fact checker.\n",
      "\n",
      "Check whether ALL claims in the answer are fully supported by the context.\n",
      "\n",
      "Rules:\n",
      "- A claim is supported ONLY if the quote logically entails it.\n",
      "- If any claim is unsupported, mark the answer as unsupported.\n",
      "\n",
      "Return JSON:\n",
      "\n",
      "{\n",
      "  \"supported\": true/false,\n",
      "  \"unsupported_claims\": [],\n",
      "  \"confidence\": <return confidence of your answer here upto the first decimal point>\n",
      "}\n",
      "\n",
      "Question:\n",
      "Which house was Harry sorted into ?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Context:\n",
      "Chunk ID - [p222_c1041] \n",
      "McGonagall’s giant chess set!’ \n",
      "At last there was silence again. \n",
      "‘Second – to Miss Hermione Granger … for the use of cool logic \n",
      "in the face of fire, I award Gryffindor house fifty points.’ \n",
      "Hermione buried her face in her arms; Harry strongly suspected \n",
      "she had burst into tears. Gryffindors up and down the table were \n",
      "beside themselves – they were a hundred points up. \n",
      "‘Third – to Mr Harry Potter …’ said Dumbledore. The room \n",
      "went deadly quiet. ‘… for pure nerve and outstanding courage, I \n",
      "award Gryffindor house sixty points.’ \n",
      "The din was deafening. Those who could add up while yelling\n",
      "\n",
      "Chunk ID - [p80_c367] \n",
      "‘Am I?’ said Harry, feeling dazed. \n",
      "‘Goodness, didn’t you know, I’d have found out everything I \n",
      "could if it was me,’ said Hermio ne. ‘Do either of you know what \n",
      "house you’ll be in? I’ve been asking around and I hope I’m in \n",
      "Gryffindor, it sounds by far the best, I hear Dumbledore himself\n",
      "\n",
      "Chunk ID - [p92_c417] \n",
      "The Sorting Hat  91 \n",
      " \n",
      "Slytherin, not Slytherin.’ \n",
      "‘Not Slytherin, eh?’ said the small voice. ‘Are you sure? You \n",
      "could be great, you know, it’s all here in your head, and Slytherin \n",
      "will help you on the way to greatn ess, no doubt about that – no? \n",
      "Well, if you’re sure – better be GRYFFINDOR!’ \n",
      "Harry heard the hat shout the last  word to the whole Hall. He \n",
      "took off the hat and walked shakily  towards the Gryffindor table. \n",
      "He was so relieved to have been chosen and not put in Slytherin, \n",
      "he hardly noticed that he was ge tting the loudest cheer yet. Percy\n",
      "\n",
      "Question:\n",
      "Which house was Harry sorted into ?\n",
      "\n",
      "Return your answer in this JSON format:\n",
      "\n",
      "{\n",
      "  \"answer\": \"...\",\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"claim\": \"...\",\n",
      "      \"evidence\": {\n",
      "        \"quote\": \"...\",\n",
      "        \"chunk_id\": \"...\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"confidence\": <return confidence of your answer here upto the first decimal point>\n",
      "}\n",
      "Answer:\n",
      "{\n",
      "  \"answer\": \"Gryffindor\",\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"claim\": \"Harry was sorted into Gryffindor\",\n",
      "      \"evidence\": {\n",
      "        \"quote\": \"The Sorting Hat  91 - Slytherin, not Slytherin.\",\n",
      "        \"chunk_id\": \"p92_c417\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"confidence\": 0.9\n",
      "}\n",
      "Answer:\n",
      "{\n",
      "  \"answer\": \"Gryffindor\",\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"claim\": \"Harry was sorted into Gryffindor\",\n",
      "      \"evidence\": {\n",
      "        \"quote\": \"The Sorting Hat  91 - Slytherin, not Slytherin.\",\n",
      "        \"chunk_id\": \"p92_c417\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"confidence\": 0.9\n",
      "}\n",
      "Answer:\n",
      "{\n",
      "  \"answer\": \"Gryffindor\",\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"claim\": \"Harry was sorted into Gryffindor\",\n",
      "      \"evidence\": {\n",
      "        \"quote\": \"The Sorting Hat  91 - Slytherin, not Slytherin.\",\n",
      "        \"chunk_id\": \"p\n",
      "\n",
      "Context:\n",
      "p222_c1041 \n",
      "McGonagall’s giant chess set!’ \n",
      "At last there was silence again. \n",
      "‘Second – to Miss Hermione Granger … for the use of cool logic \n",
      "in the face of fire, I award Gryffindor house fifty points.’ \n",
      "Hermione buried her face in her arms; Harry strongly suspected \n",
      "she had burst into tears. Gryffindors up and down the table were \n",
      "beside themselves – they were a hundred points up. \n",
      "‘Third – to Mr Harry Potter …’ said Dumbledore. The room \n",
      "went deadly quiet. ‘… for pure nerve and outstanding courage, I \n",
      "award Gryffindor house sixty points.’ \n",
      "The din was deafening. Those who could add up while yelling\n",
      "\n",
      "p80_c367 \n",
      "‘Am I?’ said Harry, feeling dazed. \n",
      "‘Goodness, didn’t you know, I’d have found out everything I \n",
      "could if it was me,’ said Hermio ne. ‘Do either of you know what \n",
      "house you’ll be in? I’ve been asking around and I hope I’m in \n",
      "Gryffindor, it sounds by far the best, I hear Dumbledore himself\n",
      "\n",
      "p92_c417 \n",
      "The Sorting Hat  91 \n",
      " \n",
      "Slytherin, not Slytherin.’ \n",
      "‘Not Slytherin, eh?’ said the small voice. ‘Are you sure? You \n",
      "could be great, you know, it’s all here in your head, and Slytherin \n",
      "will help you on the way to greatn ess, no doubt about that – no? \n",
      "Well, if you’re sure – better be GRYFFINDOR!’ \n",
      "Harry heard the hat shout the last  word to the whole Hall. He \n",
      "took off the hat and walked shakily  towards the Gryffindor table. \n",
      "He was so relieved to have been chosen and not put in Slytherin, \n",
      "he hardly noticed that he was ge tting the loudest cheer yet. Percy\n",
      "}\n",
      "\n",
      "Answer:\n",
      "{\n",
      "  \"answer\": \"Gryffindor\",\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"claim\": \"Harry was sorted into Gryffindor\",\n",
      "      \"evidence\": {\n",
      "        \"quote\": \"The Sorting Hat  91 - Slytherin, not Slytherin.\",\n",
      "        \"chunk_id\": \"p92_c417\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"confidence\": 0.9\n",
      "}\n",
      "Answer:\n",
      "{\n",
      "  \"answer\": \"Gryffindor\",\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"claim\": \"Harry was sorted into Gryffindor\",\n",
      "      \"evidence\": {\n",
      "        \"quote\": \"The Sorting Hat  91 - Slytherin, not Slytherin.\",\n",
      "        \"chunk_id\": \"p92_c417\"\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"confidence\": 0.9\n",
      "}\n",
      "\n",
      "Answer:\n",
      "{\n",
      "  \"answer\": \"Gryffindor\",\n",
      "  \"claims\": [\n",
      "    {\n",
      "      \"claim\": \"Harry was sorted into Gryffindor\",\n",
      "      \"evidence\": {\n",
      "        \"quote\": \"The Sorting Hat  91 - Slytherin, not Slytherin.\",\n",
      "        \"chunk_id\": \"\n"
     ]
    }
   ],
   "source": [
    "verification = verify_answer(question, answer, docs, llm)\n",
    "print(verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a970bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For long context handling we will map step (per chunk)\n",
    "\n",
    "map_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template = '''\n",
    "Answer the question only using the context given below \n",
    "If not relevant say \"No Relevant Information\". \n",
    "\n",
    "Context: \n",
    "{context}\n",
    "\n",
    "Question: \n",
    "{question}\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d56b2716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_steps(question, docs, llm):\n",
    "    partial_answers = [] \n",
    "    for d in docs:\n",
    "        prompt = map_prompt.format(context=d.page_content, question=question)\n",
    "        response = llm.invoke(prompt)\n",
    "        partial_answers.append((response, d.metadata))\n",
    "    return partial_answers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74291cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_prompt = PromptTemplate(input_variables=[\"partials\", \"question\"],\n",
    "                               template='''\n",
    "Combine the partial answers below into a single coherent answer. \n",
    "Cite sources and resolve contradictions \n",
    "\n",
    "Partial Answers: \n",
    "{partials}\n",
    "\n",
    "Question: \n",
    "{question}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c33849b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_step(question, partials, llm):\n",
    "    temp = \"\\n\\n\".join(f\"[{metadata['chunk_id']}] : {text}\" for text, metadata in partials)\n",
    "    prompt = reduce_prompt.format(partials=temp, question=question) \n",
    "    response = llm.invoke(prompt)\n",
    "    return response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "935fc0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combine the partial answers below into a single coherent answer. \n",
      "Cite sources and resolve contradi\n"
     ]
    }
   ],
   "source": [
    "question = \"Did Harry win his Quiditch match ?\"\n",
    "docs = compression_retriever.invoke(question)\n",
    "\n",
    "partial_answers = map_steps(question, docs, llm)\n",
    "final_answers = reduce_step(question, partial_answers, llm)\n",
    "\n",
    "print(final_answers[0:100]) #Truncating the final response due to model hallucinations and giving repeated texts back | Need inst. based models but the issue is with my personal vram limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be91adaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

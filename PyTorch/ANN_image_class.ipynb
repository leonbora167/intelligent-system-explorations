{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9dff234",
   "metadata": {},
   "source": [
    "# Prequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf5100fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import shutil\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808e644a",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c929b7",
   "metadata": {},
   "source": [
    "* **Dataset** is a python class that allows us to get the data we are supplying to the neural network\n",
    "* **Dataloader** feeds data from dataset to the neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d6465",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {}\n",
    "counter = 0\n",
    "x = []\n",
    "\n",
    "for i in tqdm(os.listdir(\"pkmn_images\")):\n",
    "    image_folder = os.path.join(\"pkmn_images\", i)\n",
    "    image_num = len(os.listdir(image_folder))\n",
    "    classes[i] = image_num\n",
    "    counter = counter + 1\n",
    "    x.append(counter)\n",
    "\n",
    "values = list(classes.values())\n",
    "\n",
    "plt.plot(x, values)\n",
    "print(\"Number of pokemon are \", len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796736f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"train\")\n",
    "os.mkdir(\"test\")\n",
    "os.mkdir(\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b77b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(os.listdir(\"pkmn_images\")):\n",
    "    class_folder = os.path.join(\"pkmn_images\", i)\n",
    "    images = os.listdir(class_folder)\n",
    "    os.mkdir(f\"train/{i}\")\n",
    "    counter = 1\n",
    "    for j in images:\n",
    "        source_path = os.path.join(class_folder, j)\n",
    "        dest_path = os.path.join(f\"train/{i}\", j)\n",
    "        shutil.move(source_path, dest_path)\n",
    "        counter = counter + 1\n",
    "        if(counter == 70):\n",
    "            break\n",
    "\n",
    "for i in tqdm(os.listdir(\"pkmn_images\")):\n",
    "    class_folder = os.path.join(\"pkmn_images\", i)\n",
    "    images = os.listdir(class_folder)\n",
    "    os.mkdir(f\"val/{i}\")\n",
    "    counter = 1\n",
    "    for j in images:\n",
    "        source_path = os.path.join(class_folder, j)\n",
    "        dest_path = os.path.join(f\"val/{i}\", j)\n",
    "        shutil.move(source_path, dest_path)\n",
    "        counter = counter + 1\n",
    "        if(counter == 10):\n",
    "            break\n",
    "\n",
    "\n",
    "for i in tqdm(os.listdir(\"pkmn_images\")):\n",
    "    class_folder = os.path.join(\"pkmn_images\", i)\n",
    "    images = os.listdir(class_folder)\n",
    "    os.mkdir(f\"test/{i}\")\n",
    "    # counter = 1\n",
    "    for j in images:\n",
    "        source_path = os.path.join(class_folder, j)\n",
    "        dest_path = os.path.join(f\"test/{i}\", j)\n",
    "        shutil.move(source_path, dest_path)\n",
    "        # counter = counter + 1\n",
    "        # if(counter == 20):\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b1c0b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#One method to get items/data from our dataset \n",
    "# Method to get the length of our dataset \n",
    "\n",
    "class Dataset:\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6a6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"train\"\n",
    "\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean = [0.485, 0.456, 0.406],\n",
    "                          std = [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "#Normalizing values between 0 and 1 makes the model a bit easier to learn the features since they are standardized or capped within a smaller range\n",
    "#Also prevents the values from getting too large during training phase - exploding gradient problem\n",
    "\n",
    "train_data = torchvision.datasets.ImageFolder(train_data_path, transform=transformations)\n",
    "\n",
    "val_data_path = \"val\"\n",
    "val_data = torchvision.datasets.ImageFolder(val_data_path, transform=transformations)\n",
    "\n",
    "test_data_path = \"test\"\n",
    "test_data = torchvision.datasets.ImageFolder(test_data_path, transform=transformations)\n",
    "\n",
    "#Training set - for training pass to update the model \n",
    "#Val set - Evaluate how model is generalizing to problem domain rather than fitting to training data not used directly\n",
    "#Test set - To get final evaluation of model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e351a260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "\n",
    "batch_size = 1024 #Num of images sent to the network once before updating it\n",
    "\n",
    "train_data_loader = DataLoader(train_data, batch_size=batch_size)\n",
    "val_data_loader = DataLoader(val_data, batch_size=batch_size)\n",
    "test_data_loader = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d54db",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9189d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our class inherits from nn.Module to \n",
    "# Register parameters, save load state dicts, move to gpu/cpu easily, wire up hooks and the __call__ -> forward mechanism\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    \n",
    "    def __init__(self):  # Defining layers and objects we want to use across the forward passes\n",
    "        super().__init__() # Initializes base nn.Module. Without this parameter registration/hook machinery wont work properly. You need super() to inherit from the parent class (nn.Module)\n",
    "        self.fc1 = nn.Linear(12288, 604)\n",
    "        self.fc2 = nn.Linear(604, 302)\n",
    "        self.fc3 = nn.Linear(302, 151)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, 1) #Flatten all dimensions except batch -> [B, 12288]\n",
    "        x = F.relu(self.fc1(x)) #linear layers expect a 2D Tensor of [Batch, Tensors] not [B, C, H, W etc]\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc3(x) #The logits are the raw tensor output from a model, preferably the last linear layers\n",
    "        #probs = logits.softmax(dim=1)\n",
    "        return logits \n",
    "    \n",
    "model = ANN() #we dont call forward() directly. We call the class which internally calss forward() via nn.Module.__call__ (also runs pre/post hooks, handles autocast etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d75718",
   "metadata": {},
   "source": [
    "Reasons for PyTorch wanting it this way is \n",
    "\n",
    "* Parameters in init -> They are registered, saved, moved to GPU and seen by optmizers. \n",
    "* Computation in forward: PyTochb builds a dynamic computation graph each call. Anything in forward is tracked by autograd for .backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6741da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = ''\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e079dd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_func, train_data_loader, val_data_loader, epochs, device):\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        training_loss = 0.0\n",
    "        training_iterator = 0\n",
    "        valid_loss = 0.0 \n",
    "        model.train() \n",
    "        for batch in train_data_loader:\n",
    "            optimizer.zero_grad()  #Refresh the optimizer for the next batch everytime \n",
    "            inputs, targets = batch \n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_func(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item()\n",
    "            training_iterator += 1\n",
    "        training_loss /= training_iterator\n",
    "        model.eval()\n",
    "        valid_iterator = 0\n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in val_data_loader:\n",
    "            inputs, targets = batch \n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_func(output, targets)\n",
    "            valid_loss += loss\n",
    "            valid_iterator += 1 \n",
    "            # preds = torch.max(output, dim=1)[1] # --> Could be a more idomatic approach\n",
    "            # correct = torch.eq(preds, target)\n",
    "            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)\n",
    "            # Taking the raw \"output\" -> logitts into probabilities across classes with softmax \n",
    "            # Picking the max indices [1] -> predicted class for each example. \n",
    "            # Torch.eq compares predicted class labels with ground truth labels returning a  boolean tensor of shape [B]\n",
    "            num_correct += torch.sum(correct).item() #Summing how many predictions were true\n",
    "            num_examples += correct.shape[0] #EQuating number of samples in each batch\n",
    "        valid_loss /= valid_iterator\n",
    "        print(f\"Epoch [{epoch}] : Training Loss = {training_loss:.2f}  Validation Loss = {valid_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10525efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068b0a29435142ccbd56c2e297ea22e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Snowwolf\\AppData\\Local\\Temp\\ipykernel_12080\\3294665301.py:33: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0] : Training Loss = 8.71  Validation Loss = 5.36\n",
      "Epoch [1] : Training Loss = 5.27  Validation Loss = 5.02\n",
      "Epoch [2] : Training Loss = 5.01  Validation Loss = 5.02\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "train(model, optimizer, loss, train_data_loader, val_data_loader, 3, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a2b8bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapidash\n"
     ]
    }
   ],
   "source": [
    "labels = {}\n",
    "for key, value in train_data.class_to_idx.items():\n",
    "    labels[value] = key \n",
    "\n",
    "img_path = \"train\\\\Pikachu\\\\24_0_5902.png\"\n",
    "img = Image.open(img_path)\n",
    "img = transformations(img)\n",
    "img = img.unsqueeze(0)\n",
    "img = img.to(device)\n",
    "\n",
    "predictions = model(img)\n",
    "prediction = predictions.argmax().item()\n",
    "print(labels[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e6c3680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"model.pkl\") #Saving the current paramters and model structure in pickle format\n",
    "# model = torch.load(\"model.pkl\")\n",
    "#Not recommended, better to save with state dict -> many benefits -> More flexibility to modify layers later on and not hardcoded to all previously trained layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f49816f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recommended path\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "model = ANN()\n",
    "model_dict = torch.load(\"model.pth\")\n",
    "model.load_state_dict(model_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a326b598",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyt-dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

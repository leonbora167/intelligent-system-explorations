{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81a30d04",
   "metadata": {},
   "source": [
    "# Why Convolutional Neural Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8dd2c5",
   "metadata": {},
   "source": [
    "* Densely connected layers learn global patterns from their input feature space\n",
    "* Convolution layers learn local patterns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c35cce",
   "metadata": {},
   "source": [
    "* The patterns they learn are transational invariant \n",
    "> Like the pattern they might learn from the lower right half of an image, same they ll be able to recognize from another corner in another image. Dense would have to learn both, making conv nets data efficient. Visual world is fundamentally transational invaraiant\n",
    "* Can learn spatial hierarchies of patterns \n",
    "> First layer might learn eyes, ears, nose. Second will learn the in depth and intricate features of each part and so on. Allowing convnets to learn increasingly complex and abstract features. World is inherently spatially hierarchial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840d557f",
   "metadata": {},
   "source": [
    "* 3D Tensors -> Feature Maps\n",
    "* Two Spatial axes (width and height) and one Depth Axis (color channel)\n",
    "* Convolution operation extracts patches from input feature and applies same transformation to all of these patches -> output feature map\n",
    "* The output feature maps depth no longer stands for the color channels rather stand for _Filters_\n",
    "* Filters encode specific aspects of the input data at a high level -> \" presenece of a face in the input \"\n",
    "* (28, 28 ,1) ---> (26, 26, 32) = Went from a feature map of size (28, 28, 1) to a feature map of size (26, 26, 32)\n",
    "* Each of the 32 output channels contains a 26x26 grid of values. Each of them are a response map of the filter over that input.\n",
    "* So every dimension in the depth map (32) is a feature (or a filter)\n",
    "* A filter map at depth \"n\" -> [:, :, n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c4c78",
   "metadata": {},
   "source": [
    "* Keras Conv2d -> (output depth, (window_height, window_width))\n",
    "* A window slides over the input feature map to produce a 1D Tensor of shape (depth,) which is then spatially patched together into a 3D output map of shape (height, width, output_depth)\n",
    "* Padding - Adds an extra approximate same number of rows and columns to either side to preserve spatial dimension\n",
    "* Stride - Distance between two succesive windows over which the filter goes on the input matrix. Normally its 1, higher stride values downsample the feature map.\n",
    "* Max Pooling - Downsamples feature maps. Instead of locally transforming the patches via a learned linear transformation like convolution they are transformed via a hardcoded max tensor operation. Usually 2x2 windows stride 2\n",
    "* Conv2d usually 3x3 window stride 1\n",
    "* We downsample the feature maps to reduce the number of parameters and spatial hierachies and patterns the successive layers or final dense layers would have to learn for each class or feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7576b146",
   "metadata": {},
   "source": [
    "# Convnet Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c81baa71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pizza images are  983\n",
      "Number of not pizza images are  983\n"
     ]
    }
   ],
   "source": [
    "#Dataset taken from https://www.kaggle.com/datasets/carlosrunner/pizza-not-pizza\n",
    "\n",
    "import os, shutil\n",
    "\n",
    "print(\"Number of pizza images are \", len(os.listdir(\"pizza_not_pizza\\\\pizza\")))\n",
    "print(\"Number of not pizza images are \", len(os.listdir(\"pizza_not_pizza\\\\not_pizza\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e09ade6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating train, val, test dataset\n",
    "\n",
    "original_dataset_dir = \"pizza_not_pizza\"\n",
    "base_dir = \"dataset\"\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "os.mkdir(train_dir)\n",
    "\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "val_dir = os.path.join(base_dir, \"val\")\n",
    "os.mkdir(val_dir)\n",
    "\n",
    "train_pizza_images_dir = os.path.join(train_dir, \"pizza\")\n",
    "os.mkdir(train_pizza_images_dir)\n",
    "train_not_pizza_images_dir = os.path.join(train_dir, \"not_pizza\")\n",
    "os.mkdir(train_not_pizza_images_dir)\n",
    "\n",
    "test_pizza_images_dir = os.path.join(test_dir, \"pizza\")\n",
    "os.mkdir(test_pizza_images_dir)\n",
    "test_not_pizza_images_dir = os.path.join(test_dir, \"not_pizza\")\n",
    "os.mkdir(test_not_pizza_images_dir)\n",
    "\n",
    "val_pizza_images_dir = os.path.join(val_dir, \"pizza\")\n",
    "os.mkdir(val_pizza_images_dir)\n",
    "val_not_pizza_images_dir = os.path.join(val_dir, \"not_pizza\")\n",
    "os.mkdir(val_not_pizza_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a251377b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import random\n",
    "\n",
    "pizza_images_list = os.listdir(\"pizza_not_pizza\\\\pizza\")\n",
    "random.shuffle(pizza_images_list)\n",
    "not_pizza_images_list = os.listdir(\"pizza_not_pizza\\\\not_pizza\")\n",
    "random.shuffle(not_pizza_images_list)\n",
    "\n",
    "train_len = 500\n",
    "val_len = 800\n",
    "test_len = 983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1991c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac581c3c2ced42cc9c55d301ea305225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Pizza Images are  500\n",
      "Testing pizza images are 183\n",
      "Validation pizza images are 300\n"
     ]
    }
   ],
   "source": [
    "for index, file in tqdm(enumerate(pizza_images_list)):\n",
    "    if(index < train_len):\n",
    "        dest_dir = os.path.join(train_pizza_images_dir, file)\n",
    "        source_dir = os.path.join(original_dataset_dir, \"pizza\", file)\n",
    "        shutil.copy(source_dir, dest_dir)\n",
    "\n",
    "    elif(index >= train_len and index < val_len):\n",
    "        dest_dir = os.path.join(val_pizza_images_dir, file)\n",
    "        source_dir = os.path.join(original_dataset_dir, \"pizza\", file)\n",
    "        shutil.copy(source_dir, dest_dir)\n",
    "\n",
    "    elif(index >= val_len and index < test_len):\n",
    "        dest_dir = os.path.join(test_pizza_images_dir, file)\n",
    "        source_dir = os.path.join(original_dataset_dir, \"pizza\", file)  \n",
    "        shutil.copy(source_dir, dest_dir)\n",
    "\n",
    "print(\"Training Pizza Images are \", len(os.listdir(train_pizza_images_dir)))\n",
    "print(\"Testing pizza images are\", len(os.listdir(test_pizza_images_dir)))\n",
    "print(\"Validation pizza images are\", len(os.listdir(val_pizza_images_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15dc16d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7772d628d1194cce9497c833c8e748cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Non Pizza Images are  500\n",
      "Testing Non pizza images are 183\n",
      "Validation Non pizza images are 300\n"
     ]
    }
   ],
   "source": [
    "for index, file in tqdm(enumerate(not_pizza_images_list)):\n",
    "    if(index < train_len):\n",
    "        dest_dir = os.path.join(train_not_pizza_images_dir, file)\n",
    "        source_dir = os.path.join(original_dataset_dir, \"not_pizza\", file)\n",
    "        shutil.copy(source_dir, dest_dir)\n",
    "\n",
    "    elif(index >= train_len and index < val_len):\n",
    "        dest_dir = os.path.join(val_not_pizza_images_dir, file)\n",
    "        source_dir = os.path.join(original_dataset_dir, \"not_pizza\", file)\n",
    "        shutil.copy(source_dir, dest_dir)\n",
    "\n",
    "    elif(index >= val_len and index < test_len):\n",
    "        dest_dir = os.path.join(test_not_pizza_images_dir, file)\n",
    "        source_dir = os.path.join(original_dataset_dir, \"not_pizza\", file)  \n",
    "        shutil.copy(source_dir, dest_dir)\n",
    "\n",
    "print(\"Training Non Pizza Images are \", len(os.listdir(train_not_pizza_images_dir)))\n",
    "print(\"Testing Non pizza images are\", len(os.listdir(test_not_pizza_images_dir)))\n",
    "print(\"Validation Non pizza images are\", len(os.listdir(val_not_pizza_images_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53601d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "982"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27419cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

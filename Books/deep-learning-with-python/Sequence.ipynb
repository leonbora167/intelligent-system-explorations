{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd2032f5",
   "metadata": {},
   "source": [
    "* For sequence data the conventional dl algorithms are *Recurrent Neural Networks* and *1D Convnets*\n",
    "> Document Classification, Timeseries classification\n",
    "\n",
    "> Timseries comparison\n",
    "\n",
    "> Sequence to sequence mapping\n",
    "\n",
    "> Sentiment Analysis\n",
    "\n",
    "> Timeseries Forecasting \n",
    "\n",
    "> ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f95e53b",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e602af52",
   "metadata": {},
   "source": [
    "* Either a sequence of characters or sequence of words\n",
    "* More common to work word level\n",
    "* Do not understand the text as per human understanding. \n",
    "* Rather map the statistical structure of written language. \n",
    "* Pattern recognition for words - sentences - paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59011dc",
   "metadata": {},
   "source": [
    "* Either words -> vectors\n",
    "* Characters -> vectors\n",
    "* groups (n-grams) of words -> vectors\n",
    "* Each unit we can break down text is called token \n",
    "* Process of breaking text -> tokens = Tokenization\n",
    "* Tokens -> Vectors -> Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ce1205",
   "metadata": {},
   "source": [
    "* In Bag of Words, tokenization is not order preserving\n",
    "* Sequence of tokens is not fixed\n",
    "* Tokens are understood as a set as general structure of sentence is lost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabe3b40",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95099990",
   "metadata": {},
   "source": [
    "* Associating a unique integer index with every word\n",
    "* Then turning these integers into a vector of size N (vocab size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "999c763a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "samples = [\"The cat sat on the mat\", 'Hello', 'Dog is funny']\n",
    "token_index = {}\n",
    "for sample in samples:\n",
    "    for word in sample.split():\n",
    "        if word not in token_index:\n",
    "            token_index[word] = len(token_index) + 1\n",
    "\n",
    "max_length = 10 #Think this is arbitraly setting a maximum length\n",
    "results = np.zeros(shape=(len(samples), max_length, max(token_index.values())+1))\n",
    "#results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9032fae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unqiue tokens are : 9\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "\n",
    "samples = [\"The cat sat on the mat\", 'Hello', 'Dog is funny']\n",
    "tokenizer = Tokenizer(num_words=1000) #Configured for most common 1000 words\n",
    "tokenizer.fit_on_texts(samples)\n",
    "sequences = tokenizer.texts_to_sequences(samples)\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(samples, mode=\"binary\")\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print(\"Number of unqiue tokens are :\", len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41132b9b",
   "metadata": {},
   "source": [
    "# Word Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee7dc7",
   "metadata": {},
   "source": [
    "* Associate vector with a word\n",
    "* Dense vectors <-> Word embeddings\n",
    "* Vectors obtained through one hot encoding are binray with lots of spare representations leading to high dimensionality\n",
    "* Word vectors are low dimensionality floating point vectors \n",
    "* Word embeddings pack far more information into less dimensional vectors than sparse vectors\n",
    "* Sparse vectors are hardcoded while embeddings are learnt by the model from a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab71cb2",
   "metadata": {},
   "source": [
    "* Either we use a pre trained embedding model.\n",
    "* Train the embedding model on the dataset we want to use for our application. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96478bc0",
   "metadata": {},
   "source": [
    "* Geometric relationships between different words should also reflect in their semantic differences\n",
    "* Words embeddings are meant to map human meanings of them into a geometrical space\n",
    "* Thats the word embedding phase\n",
    "* The geometric distance between words should reflect their semantic differences\n",
    "* Related words closer and non similar words farther apart as per their distances\n",
    "* Also geometric transformations are supported -> \"King\" + \"Woman\" = \"Queen\" + \"plural\" = \"Queens\"\n",
    "* Word embeddings spaces depend on language and/or task.\n",
    "* What's good for an English task might not be good for Chinese.\n",
    "* Importance of certain semantic relations varies from task to task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce19d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A new embedding space with every new task\n",
    "\n",
    "from keras.layers import Embedding\n",
    "\n",
    "embedding_layer = Embedding(1000, 64) # (number of possible tokens, dims of embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ce71f5",
   "metadata": {},
   "source": [
    "* Word index -> Embedding Layer -> Corresponding Word Vector\n",
    "* Embedding layer essentially a dictionary that maps integer indexes to dense vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7aaf719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

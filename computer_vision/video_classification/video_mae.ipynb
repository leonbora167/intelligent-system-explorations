{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83d6d217",
   "metadata": {},
   "source": [
    "# Prequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "989114ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from boxmot import BoostTrack\n",
    "from boxmot import StrongSort\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Lambda, Normalize, CenterCrop, Resize, ToTensor\n",
    "from torchvision.transforms._transforms_video import CenterCropVideo, NormalizeVideo\n",
    "import cv2\n",
    "\n",
    "# from pytorchvideo.transforms import ShortSideScale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d78959",
   "metadata": {},
   "source": [
    "# Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "987f1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_model = YOLO(\"yolo11s.pt\")\n",
    "\n",
    "def object_detection(img):\n",
    "    lis = []\n",
    "    results = detector_model.predict(img, verbose=False, device=\"cuda\")\n",
    "    temp = img.copy()\n",
    "    for result in results:\n",
    "        pass\n",
    "    length = len(result.boxes.data)\n",
    "    for index in range(length):\n",
    "        boxes = result.boxes\n",
    "        xyxy = boxes.xyxy[index].detach().cpu()\n",
    "        x1, y1, x2, y2 = int(xyxy[0].item()), int(xyxy[1].item()), int(xyxy[2].item()), int(xyxy[3].item())\n",
    "        # xywh = boxes.xywh[0].detach().cpu()\n",
    "        # x1, y1, w, h = int(xywh[0].item()), int(xywh[1].item()), int(xywh[2].item()), int(xywh[3].item())\n",
    "        cls_id = int(result.boxes.cls[index].detach().cpu().item())\n",
    "        if(cls_id != 0): #Only person detection trying\n",
    "            continue\n",
    "        confidence = float(result.boxes.conf[index].detach().cpu().item())\n",
    "        cv2.rectangle(temp, (x1, y1), (x2, y2), (255,0,0), 2)\n",
    "        cv2.putText(temp, text=\"Person\", org=(x1, y1-5), color=(255,0,0), fontScale=0.2, fontFace=cv2.FONT_HERSHEY_DUPLEX)\n",
    "        res = [x1, y1, x2, y2, confidence, cls_id]\n",
    "        lis.append(res)\n",
    "    return lis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2bdf7c6",
   "metadata": {},
   "source": [
    "# Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fc1bf78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-06 00:16:20.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mBaseTracker initialization parameters:\u001b[0m\n",
      "\u001b[32m2025-10-06 00:16:20.269\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mdet_thresh: 0.3\u001b[0m\n",
      "\u001b[32m2025-10-06 00:16:20.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mmax_age: 30\u001b[0m\n",
      "\u001b[32m2025-10-06 00:16:20.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mmax_obs: 50\u001b[0m\n",
      "\u001b[32m2025-10-06 00:16:20.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mmin_hits: 3\u001b[0m\n",
      "\u001b[32m2025-10-06 00:16:20.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1miou_threshold: 0.3\u001b[0m\n",
      "\u001b[32m2025-10-06 00:16:20.271\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mper_class: False\u001b[0m\n",
      "\u001b[32m2025-10-06 00:16:20.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mnr_classes: 80\u001b[0m\n",
      "\u001b[32m2025-10-06 00:16:20.272\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1masso_func: iou\u001b[0m\n",
      "\u001b[32m2025-10-06 00:16:20.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mis_obb: False\u001b[0m\n",
      "\u001b[32m2025-10-06 00:16:20.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mAdditional parameters:\u001b[0m\n",
      "\u001b[32m2025-10-06 00:16:20.273\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mcmc: False\u001b[0m\n",
      "\u001b[32m2025-10-06 00:16:20.274\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.utils.torch_utils\u001b[0m:\u001b[36mselect_device\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mYolo Tracking v15.0.2 ðŸš€ Python-3.9.23 torch-2.8.0+cu129CPU\u001b[0m\n",
      "\u001b[32m2025-10-06 00:16:20.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.appearance.backends.base_backend\u001b[0m:\u001b[36mdownload_model\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1m[PID 3268] Found existing ReID weights at osnet_x0_25_market1501.pt; skipping download.\u001b[0m\n",
      "\u001b[32m2025-10-06 00:16:20.340\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid.registry\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m67\u001b[0m - \u001b[32m\u001b[1mLoaded pretrained weights from osnet_x0_25_market1501.pt\u001b[0m\n",
      "\u001b[32m2025-10-06 00:16:20.351\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.trackers.strongsort.strongsort\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m111\u001b[0m - \u001b[32m\u001b[1mInitialized StrongSort\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # device = torch.device(\"cuda\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "#tracker = BoostTrack(reid_weights='osnet_x0_25_msmt17.pt', device=device, half=False)\n",
    "\n",
    "tracker = StrongSort(reid_weights=Path(\"osnet_x0_25_market1501.pt\"), device=\"cpu\", \n",
    "                     half=False, cmc=False) #CMC is motion compensation\n",
    "\n",
    "def visualize_tracking(tracker_results, img):\n",
    "    for i in tracker_results:\n",
    "        x1, y1, x2, y2 = int(i[0]), int(i[1]), int(i[2]), int(i[3])\n",
    "        track_id = int(i[4])\n",
    "        track_confidence = float(i[5])\n",
    "        cv2.rectangle(img, (x1,y1), (x2, y2), (255, 0, 0), 2)\n",
    "        cv2.putText(img, text=str(track_id), org=(x1, y1-5), color=(255,0,0), fontScale=0.2, fontFace=cv2.FONT_HERSHEY_DUPLEX)\n",
    "    return img \n",
    "\n",
    "def save_video_clip(track__buffers, out_path, track_id):\n",
    "    h, w, _ = track__buffers[track_id][0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(out_path, fourcc, 15, (w, h))\n",
    "    for f in track__buffers[track_id]:\n",
    "        out.write(cv2.cvtColor(f, cv2.COLOR_BGR2RGB))\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd16efe",
   "metadata": {},
   "source": [
    "# Video MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c34376f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "processor = VideoMAEImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\", cache_dir=\"models\")\n",
    "video_model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\", cache_dir=\"models\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0243cb",
   "metadata": {},
   "source": [
    "# Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563f07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    Lambda(lambda x: x / 255.0),\n",
    "    CenterCropVideo(256),\n",
    "    NormalizeVideo((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c9f1061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_action_recognition(track_buffer):\n",
    "    # track_buffer: list of frames (each frame is H x W x 3)\n",
    "    clip = np.stack(track_buffer)  # (T, H, W, 3)\n",
    "    clip = clip.transpose(0, 3, 1, 2)  # (T, 3, H, W)\n",
    "    #print(clip.shape)\n",
    "    # Convert to list of torch tensors\n",
    "    video = [frame for frame in clip]\n",
    "\n",
    "    # Process with the VideoMAE processor\n",
    "    inputs = processor(video, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = video_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_class_idx = logits.argmax(-1).item()\n",
    "        predicted_label = video_model.config.id2label[predicted_class_idx]\n",
    "        confidence = logits[0, predicted_class_idx].item()\n",
    "\n",
    "\n",
    "    return predicted_label, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084d21fc",
   "metadata": {},
   "source": [
    "# With Tracker Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "831005be",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"video1.mp4\"\n",
    "cap = cv2.VideoCapture(path)\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "output_path = \"VideoMAE-wt-4.mp4\"\n",
    "\n",
    "h, w = frame_height, frame_width \n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# Initialize tqdm progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf9f075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff57886830184892b187fb93b18966d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing video:   0%|          | 0/300 [00:00<?, ?frame/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ended\n"
     ]
    }
   ],
   "source": [
    "crop_size = 256\n",
    "track__buffers = defaultdict(list)\n",
    "save_dir = \"clips\"\n",
    "frame_idx = 0\n",
    "pred_class = \"-\"\n",
    "pbar = tqdm(total=total_frames, desc=\"Processing video\", unit=\"frame\")\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video ended\")\n",
    "        break \n",
    "    frame_idx += 1\n",
    "    pbar.update(1)\n",
    "    # if(frame_idx % 3 != 0):\n",
    "    #     continue\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    copy_frame = frame.copy()\n",
    "    detections = np.array(object_detection(frame))\n",
    "    if(detections.size == 0):\n",
    "        out.write(frame)\n",
    "        continue\n",
    "    tracker_results = tracker.update(detections, frame)\n",
    "    #print(tracker_results)\n",
    "    #annotated_img = visualize_tracking(tracker_results, frame)\n",
    "    #Cropping separately as per id\n",
    "    if(tracker_results.size == 0):\n",
    "        out.write(frame)\n",
    "        continue\n",
    "    for x1,y1,x2,y2, track_id, conf, cls_id, frame_id in tracker_results:\n",
    "        if(x2-x1 < 10 and y2-y1 < 10):\n",
    "            out.write(frame)\n",
    "            continue\n",
    "        x1,y1,x2,y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "        crop = copy_frame[y1:y2, x1:x2]\n",
    "        if crop.size == 0 or crop.shape[0] == 0 or crop.shape[1] == 0: #For crop being empty or invalid\n",
    "            out.write(frame)\n",
    "            continue\n",
    "        crop = cv2.resize(crop, (crop_size, crop_size))\n",
    "        track__buffers[track_id].append(crop)\n",
    "        cv2.rectangle(frame, (x1,y1), (x2, y2), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, text=str(track_id), org=(x1, y1-5), color=(255,0,0), fontScale=0.5, fontFace=cv2.FONT_HERSHEY_PLAIN, thickness=1)\n",
    "        cv2.putText(frame, text=str(pred_class), org=(x1+30, y1-5), color=(255,0,0), fontScale=0.5, fontFace=cv2.FONT_HERSHEY_PLAIN, thickness=1)\n",
    "\n",
    "        if len(track__buffers[track_id]) == 16:\n",
    "            out_path = os.path.join(save_dir, f\"id_{int(track_id)}_frame_{int(frame_idx)}.mp4\")\n",
    "            #save_video_clip(track__buffers, out_path, track_id)\n",
    "            preds = run_action_recognition(track__buffers[track_id])\n",
    "            #print(preds)\n",
    "            pred_class = preds[0]\n",
    "            confidence = preds[1]\n",
    "            track__buffers[track_id].clear()\n",
    "    out.write(frame)\n",
    "    # cv2.imshow(\"Tracker_frame\", frame)\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #     break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b50a5a",
   "metadata": {},
   "source": [
    "# Withoutt Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c6a9cd1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"car.mp4\"\n",
    "cap = cv2.VideoCapture(path)\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "output_path = \"VideoMAE-2.mp4\"\n",
    "\n",
    "h, w = frame_height, frame_width \n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# Initialize tqdm progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d6cc4aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4e1305bba9641abb88791fae8588a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing video:   0%|          | 0/839 [00:00<?, ?frame/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ended\n"
     ]
    }
   ],
   "source": [
    "crop_size = 256\n",
    "track__buffers = defaultdict(list)\n",
    "save_dir = \"clips\"\n",
    "frame_idx = 0\n",
    "pred_class = \"-\"\n",
    "\n",
    "pbar = tqdm(total=total_frames, desc=\"Processing video\", unit=\"frame\")\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video ended\")\n",
    "        break \n",
    "    frame_idx += 1\n",
    "    pbar.update(1)\n",
    "    # if(frame_idx % 3 != 0):\n",
    "    #     continue\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    cv2.putText(frame, text=str(pred_class), org=(40, 40), color=(0,0,255), fontScale=2, fontFace=cv2.FONT_HERSHEY_SIMPLEX, thickness=2)\n",
    "    img = cv2.resize(frame, (256, 256))\n",
    "    track__buffers[0].append(img)\n",
    "    if len(track__buffers[0]) == 16:\n",
    "        preds = run_action_recognition(track__buffers[0])\n",
    "        #print(preds)\n",
    "        pred_class = preds[0]\n",
    "        confidence = preds[1]\n",
    "        cv2.putText(frame, text=str(pred_class), org=(40, 40), color=(0,0,255), fontScale=2, fontFace=cv2.FONT_HERSHEY_SIMPLEX, thickness=2)\n",
    "        out.write(frame)\n",
    "        track__buffers[0].clear()\n",
    "    out.write(frame)\n",
    "    # cv2.imshow(\"Tracker_frame\", frame)\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #     break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231e4c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slowfast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

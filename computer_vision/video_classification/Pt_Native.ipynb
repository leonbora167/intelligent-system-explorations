{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af4a483e",
   "metadata": {},
   "source": [
    "# Prequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5078a9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from boxmot import BoostTrack\n",
    "from boxmot import StrongSort\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import Compose, Lambda, Normalize, CenterCrop, Resize, ToTensor\n",
    "from torchvision.transforms._transforms_video import CenterCropVideo, NormalizeVideo\n",
    "import cv2\n",
    "\n",
    "# from pytorchvideo.transforms import ShortSideScale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d4fe6a",
   "metadata": {},
   "source": [
    "# Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80b6408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector_model = YOLO(\"yolo11s.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7c5a4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection(img):\n",
    "    lis = []\n",
    "    results = detector_model.predict(img, verbose=False, device=\"cuda\")\n",
    "    temp = img.copy()\n",
    "    for result in results:\n",
    "        pass\n",
    "    length = len(result.boxes.data)\n",
    "    for index in range(length):\n",
    "        boxes = result.boxes\n",
    "        xyxy = boxes.xyxy[index].detach().cpu()\n",
    "        x1, y1, x2, y2 = int(xyxy[0].item()), int(xyxy[1].item()), int(xyxy[2].item()), int(xyxy[3].item())\n",
    "        # xywh = boxes.xywh[0].detach().cpu()\n",
    "        # x1, y1, w, h = int(xywh[0].item()), int(xywh[1].item()), int(xywh[2].item()), int(xywh[3].item())\n",
    "        cls_id = int(result.boxes.cls[index].detach().cpu().item())\n",
    "        if(cls_id != 0): #Only person detection trying\n",
    "            continue\n",
    "        confidence = float(result.boxes.conf[index].detach().cpu().item())\n",
    "        cv2.rectangle(temp, (x1, y1), (x2, y2), (255,0,0), 2)\n",
    "        cv2.putText(temp, text=\"Person\", org=(x1, y1-5), color=(255,0,0), fontScale=0.2, fontFace=cv2.FONT_HERSHEY_DUPLEX)\n",
    "        res = [x1, y1, x2, y2, confidence, cls_id]\n",
    "        lis.append(res)\n",
    "    return lis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d702a1d8",
   "metadata": {},
   "source": [
    "# Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75f1d5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-05 20:31:04.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m56\u001b[0m - \u001b[1mBaseTracker initialization parameters:\u001b[0m\n",
      "\u001b[32m2025-10-05 20:31:04.005\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mdet_thresh: 0.3\u001b[0m\n",
      "\u001b[32m2025-10-05 20:31:04.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m58\u001b[0m - \u001b[1mmax_age: 30\u001b[0m\n",
      "\u001b[32m2025-10-05 20:31:04.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m59\u001b[0m - \u001b[1mmax_obs: 50\u001b[0m\n",
      "\u001b[32m2025-10-05 20:31:04.006\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m60\u001b[0m - \u001b[1mmin_hits: 3\u001b[0m\n",
      "\u001b[32m2025-10-05 20:31:04.007\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1miou_threshold: 0.3\u001b[0m\n",
      "\u001b[32m2025-10-05 20:31:04.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m62\u001b[0m - \u001b[1mper_class: False\u001b[0m\n",
      "\u001b[32m2025-10-05 20:31:04.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mnr_classes: 80\u001b[0m\n",
      "\u001b[32m2025-10-05 20:31:04.008\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m64\u001b[0m - \u001b[1masso_func: iou\u001b[0m\n",
      "\u001b[32m2025-10-05 20:31:04.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m65\u001b[0m - \u001b[1mis_obb: False\u001b[0m\n",
      "\u001b[32m2025-10-05 20:31:04.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mAdditional parameters:\u001b[0m\n",
      "\u001b[32m2025-10-05 20:31:04.009\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.trackers.basetracker\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m69\u001b[0m - \u001b[1mcmc: False\u001b[0m\n",
      "\u001b[32m2025-10-05 20:31:04.011\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.utils.torch_utils\u001b[0m:\u001b[36mselect_device\u001b[0m:\u001b[36m78\u001b[0m - \u001b[1mYolo Tracking v15.0.2 ðŸš€ Python-3.9.23 torch-2.8.0+cu129CPU\u001b[0m\n",
      "\u001b[32m2025-10-05 20:31:04.012\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mboxmot.appearance.backends.base_backend\u001b[0m:\u001b[36mdownload_model\u001b[0m:\u001b[36m153\u001b[0m - \u001b[1m[PID 49520] Found existing ReID weights at osnet_x0_25_market1501.pt; skipping download.\u001b[0m\n",
      "\u001b[32m2025-10-05 20:31:04.096\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.appearance.reid.registry\u001b[0m:\u001b[36mload_pretrained_weights\u001b[0m:\u001b[36m67\u001b[0m - \u001b[32m\u001b[1mLoaded pretrained weights from osnet_x0_25_market1501.pt\u001b[0m\n",
      "\u001b[32m2025-10-05 20:31:04.107\u001b[0m | \u001b[32m\u001b[1mSUCCESS \u001b[0m | \u001b[36mboxmot.trackers.strongsort.strongsort\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m111\u001b[0m - \u001b[32m\u001b[1mInitialized StrongSort\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # device = torch.device(\"cuda\")\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "#tracker = BoostTrack(reid_weights='osnet_x0_25_msmt17.pt', device=device, half=False)\n",
    "\n",
    "tracker = StrongSort(reid_weights=Path(\"osnet_x0_25_market1501.pt\"), device=\"cpu\", \n",
    "                     half=False, cmc=False) #CMC is motion compensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "738b33e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tracking(tracker_results, img):\n",
    "    for i in tracker_results:\n",
    "        x1, y1, x2, y2 = int(i[0]), int(i[1]), int(i[2]), int(i[3])\n",
    "        track_id = int(i[4])\n",
    "        track_confidence = float(i[5])\n",
    "        cv2.rectangle(img, (x1,y1), (x2, y2), (255, 0, 0), 2)\n",
    "        cv2.putText(img, text=str(track_id), org=(x1, y1-5), color=(255,0,0), fontScale=0.2, fontFace=cv2.FONT_HERSHEY_DUPLEX)\n",
    "    return img \n",
    "\n",
    "def save_video_clip(track__buffers, out_path, track_id):\n",
    "    h, w, _ = track__buffers[track_id][0].shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(out_path, fourcc, 15, (w, h))\n",
    "    for f in track__buffers[track_id]:\n",
    "        out.write(cv2.cvtColor(f, cv2.COLOR_BGR2RGB))\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a712fd45",
   "metadata": {},
   "source": [
    "# Resnet 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c706f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Snowwolf\\miniconda3\\envs\\slowfast\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "video_model = torchvision.models.video.r3d_18(weights=torchvision.models.video.R3D_18_Weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "33c94881",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "    Lambda(lambda x: x / 255.0),\n",
    "    CenterCropVideo(256),\n",
    "    NormalizeVideo((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "58a1d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_action_recognition(track_buffer):\n",
    "    clip = np.stack(track_buffer) # (T, H, W, 3) -> (32, H, W, 3)\n",
    "    clip = torch.from_numpy(clip).permute(3,0,1,2) # [C, T, H, W]\n",
    "\n",
    "    clip = transform(clip)\n",
    "\n",
    "    inputs = clip.unsqueeze(0).float() #Adding batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = video_model(inputs)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "250e429e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torchvision.models.video.R3D_18_Weights.DEFAULT\n",
    "categories = weights.meta[\"categories\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59602339",
   "metadata": {},
   "source": [
    "## With Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "93fdb37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"video1.mp4\"\n",
    "cap = cv2.VideoCapture(path)\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "output_path = \"Resnet3D-wt-4.mp4\"\n",
    "\n",
    "h, w = frame_height, frame_width \n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "# Initialize tqdm progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "363734e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33389eb376a14929abcb96eae6d236fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing video:   0%|          | 0/300 [00:00<?, ?frame/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ended\n"
     ]
    }
   ],
   "source": [
    "crop_size = 256\n",
    "track__buffers = defaultdict(list)\n",
    "save_dir = \"clips\"\n",
    "frame_idx = 0\n",
    "pred_class = \"-\"\n",
    "pbar = tqdm(total=total_frames, desc=\"Processing video\", unit=\"frame\")\n",
    "\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video ended\")\n",
    "        break \n",
    "    frame_idx += 1\n",
    "    pbar.update(1)\n",
    "    # if(frame_idx % 3 != 0):\n",
    "    #     continue\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    copy_frame = frame.copy()\n",
    "    detections = np.array(object_detection(frame))\n",
    "    if(detections.size == 0):\n",
    "        out.write(frame)\n",
    "        continue\n",
    "    tracker_results = tracker.update(detections, frame)\n",
    "    #print(tracker_results)\n",
    "    #annotated_img = visualize_tracking(tracker_results, frame)\n",
    "    #Cropping separately as per id\n",
    "    if(tracker_results.size == 0):\n",
    "        out.write(frame)\n",
    "        continue\n",
    "    for x1,y1,x2,y2, track_id, conf, cls_id, frame_id in tracker_results:\n",
    "        if(x2-x1 < 10 and y2-y1 < 10):\n",
    "            out.write(frame)\n",
    "            continue\n",
    "        x1,y1,x2,y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "        crop = copy_frame[y1:y2, x1:x2]\n",
    "        if crop.size == 0 or crop.shape[0] == 0 or crop.shape[1] == 0: #For crop being empty or invalid\n",
    "            out.write(frame)\n",
    "            continue\n",
    "        crop = cv2.resize(crop, (crop_size, crop_size))\n",
    "        track__buffers[track_id].append(crop)\n",
    "        cv2.rectangle(frame, (x1,y1), (x2, y2), (255, 0, 0), 2)\n",
    "        cv2.putText(frame, text=str(track_id), org=(x1, y1-5), color=(255,0,0), fontScale=1, fontFace=cv2.FONT_HERSHEY_COMPLEX)\n",
    "        cv2.putText(frame, text=str(pred_class), org=(x1+15, y1-5), color=(255,0,0), fontScale=2, fontFace=cv2.FONT_HERSHEY_COMPLEX)\n",
    "\n",
    "        if len(track__buffers[track_id]) == 32:\n",
    "            out_path = os.path.join(save_dir, f\"id_{int(track_id)}_frame_{int(frame_idx)}.mp4\")\n",
    "            #save_video_clip(track__buffers, out_path, track_id)\n",
    "            preds = run_action_recognition(track__buffers[track_id])\n",
    "            probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "            top_prob, top_class = torch.max(probs, dim=1)\n",
    "            pred_class = categories[top_class]\n",
    "            prob = top_prob.item()\n",
    "            track__buffers[track_id] = []\n",
    "    out.write(frame)\n",
    "    # cv2.imshow(\"Tracker_frame\", frame)\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #     break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "668ee99e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(85, 0, 3), dtype=uint8)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd77c45",
   "metadata": {},
   "source": [
    "# Without Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d321f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"video1.mp4\"\n",
    "cap = cv2.VideoCapture(path)\n",
    "\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "output_path = \"Resnet3D-4.mp4\"\n",
    "\n",
    "h, w = frame_height, frame_width \n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
    "\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "99fb8c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997d2df3f7a64c0c8da1f807511a6913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing video:   0%|          | 0/300 [00:00<?, ?frame/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video ended\n"
     ]
    }
   ],
   "source": [
    "save_dir = \"clips\"\n",
    "frame_idx = 0\n",
    "pred_class = \"-\"\n",
    "\n",
    "pbar = tqdm(total=total_frames, desc=\"Processing video\", unit=\"frame\")\n",
    "while True:\n",
    "    success, frame = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video ended\")\n",
    "        break \n",
    "    frame_idx += 1\n",
    "    pbar.update(1)\n",
    "    # if(frame_idx % 3 != 0):\n",
    "    #     continue\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    cv2.putText(frame, text=str(pred_class), org=(40, 40), color=(0,0,255), fontScale=2, fontFace=cv2.FONT_HERSHEY_COMPLEX)\n",
    "    out.write(frame)\n",
    "    img = cv2.resize(frame, (256, 256))\n",
    "    track__buffers[0].append(img)\n",
    "    if len(track__buffers[0]) == 32:\n",
    "        preds = run_action_recognition(track__buffers[0])\n",
    "        probs = torch.nn.functional.softmax(preds, dim=1)\n",
    "        top_prob, top_class = torch.max(probs, dim=1)\n",
    "        pred_class = categories[top_class]\n",
    "        cv2.putText(frame, text=str(pred_class), org=(40, 40), color=(0,0,255), fontScale=2, fontFace=cv2.FONT_HERSHEY_COMPLEX)\n",
    "        prob = top_prob.item()\n",
    "        out.write(frame)\n",
    "        track__buffers[0] = []\n",
    "    # cv2.imshow(\"Tracker_frame\", frame)\n",
    "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #     break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195a8ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "slowfast",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
